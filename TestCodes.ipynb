{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d614f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_orders():\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv('orders_2.csv')\n",
    "    df['datetime'] = pd.to_datetime(df[\"InvoiceDate\"], format='%m/%d/%Y %H:%M')\n",
    "    df.drop('InvoiceDate', axis=1, inplace=True)\n",
    "    df = df.rename(columns={'datetime': 'InvoiceDate'})\n",
    "\n",
    "    #Second Orders file\n",
    "    df2 = pd.read_csv('dataset/orders.csv')\n",
    "    df2['date_column'] = pd.to_datetime(df2['InvoiceDate'], format='%Y/%m/%d %H:%M:%S', errors='coerce')\n",
    "    df2.drop('InvoiceDate', axis=1, inplace=True)\n",
    "    df2 = df2.rename(columns={'date_column': 'InvoiceDate'})\n",
    "\n",
    "    #combine both orders\n",
    "    sales = pd.concat([df, df2], ignore_index=True)\n",
    "    sales.to_csv('dataset/orders.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "127d4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Global Variables\n",
    "postgresql_engine = create_engine(\"postgresql://growth:growth-school@localhost:5432/GrediStore\")\n",
    "\n",
    "postgresql_conn = postgresql_engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "        \n",
    "    # Setup the Open-Meteo API client with cache and retry on error\n",
    "    cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
    "    retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "    openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "    # \"Morehead, KY, USA\"\n",
    "    latitude = -83.432686\n",
    "    longitude = 38.183971\n",
    "\n",
    "    # Check if latitude and longitude are within valid ranges\n",
    "    if -90 <= latitude <= 90 and -180 <= longitude <= 180:\n",
    "\n",
    "        # Set latitude and longitude in params\n",
    "        params = {\n",
    "            \"latitude\": latitude,\n",
    "            \"longitude\": longitude,\n",
    "            \"start_date\": \"2010-01-01\",\n",
    "            \"end_date\": \"2011-10-31\",\n",
    "            \"hourly\": [\n",
    "                \"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\",\n",
    "                \"apparent_temperature\", \"precipitation\", \"rain\", \"snowfall\",\n",
    "                \"snow_depth\", \"weather_code\", \"pressure_msl\", \"surface_pressure\",\n",
    "                \"cloud_cover\", \"wind_speed_10m\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        # The order of variables in hourly or daily is important to assign them correctly below\n",
    "        url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "        \n",
    "        responses = openmeteo.weather_api(url, params=params)\n",
    "        response = responses[0]\n",
    "\n",
    "        # Process hourly data. The order of variables needs to be the same as requested.\n",
    "        hourly = response.Hourly()\n",
    "        hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "        hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
    "        hourly_dew_point_2m = hourly.Variables(2).ValuesAsNumpy()\n",
    "        hourly_apparent_temperature = hourly.Variables(3).ValuesAsNumpy()\n",
    "        hourly_precipitation = hourly.Variables(4).ValuesAsNumpy()\n",
    "        hourly_rain = hourly.Variables(5).ValuesAsNumpy()\n",
    "        hourly_snowfall = hourly.Variables(6).ValuesAsNumpy()\n",
    "        hourly_snow_depth = hourly.Variables(7).ValuesAsNumpy()\n",
    "        hourly_weather_code = hourly.Variables(8).ValuesAsNumpy()\n",
    "        hourly_pressure_msl = hourly.Variables(9).ValuesAsNumpy()\n",
    "        hourly_surface_pressure = hourly.Variables(10).ValuesAsNumpy()\n",
    "        hourly_cloud_cover = hourly.Variables(11).ValuesAsNumpy()\n",
    "        hourly_wind_speed_10m = hourly.Variables(12).ValuesAsNumpy()\n",
    "\n",
    "        hourly_data = {\"date\": pd.date_range(\n",
    "            start = pd.to_datetime(hourly.Time(), unit = \"s\"),\n",
    "            end = pd.to_datetime(hourly.TimeEnd(), unit = \"s\"),\n",
    "            freq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "            inclusive = \"left\"\n",
    "        )}\n",
    "        hourly_data[\"temp\"] = hourly_temperature_2m\n",
    "        hourly_data[\"rel_hum\"] = hourly_relative_humidity_2m\n",
    "        hourly_data[\"dew_pnt\"] = hourly_dew_point_2m\n",
    "        hourly_data[\"apparent_temp\"] = hourly_apparent_temperature\n",
    "        hourly_data[\"precip\"] = hourly_precipitation\n",
    "        hourly_data[\"rain\"] = hourly_rain\n",
    "        hourly_data[\"snowfall\"] = hourly_snowfall\n",
    "        hourly_data[\"snow_depth\"] = hourly_snow_depth\n",
    "        hourly_data[\"weather_code\"] = hourly_weather_code\n",
    "        hourly_data[\"pressure_msl\"] = hourly_pressure_msl\n",
    "        hourly_data[\"surface_pressure\"] = hourly_surface_pressure\n",
    "        hourly_data[\"cloud_cover\"] = hourly_cloud_cover\n",
    "        hourly_data[\"wind_speed\"] = hourly_wind_speed_10m\n",
    "        hourly_data[\"latitude\"] = latitude\n",
    "        hourly_data[\"longitde\"] = longitude\n",
    "\n",
    "        hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "    hourly_dataframe.to_csv('dataset/weathers.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_postgresql_tables():\n",
    "    # Create tables in PostgreSQL (replace with actual table names and schema)\n",
    "    \n",
    "    postgresql_conn.execute('''\n",
    "        DROP TABLE IF EXISTS growth_schema.customers CASCADE;\n",
    "        CREATE TABLE IF NOT EXISTS growth_schema.customers (\n",
    "            customer_id CHAR(5) PRIMARY KEY,\n",
    "            first_name VARCHAR(255) NOT NULL,\n",
    "            last_name VARCHAR(255) NOT NULL,\n",
    "            date_of_birth DATE NOT NULL,\n",
    "            email  VARCHAR(255) NOT NULL UNIQUE,\n",
    "            gender VARCHAR(6) CHECK(gender IN ('Female', 'Male')),\n",
    "            city VARCHAR(255) NOT NULL,\n",
    "            age CHAR(2) NOT NULL,\n",
    "            address VARCHAR(255) NOT NULL,\n",
    "            latitude DOUBLE PRECISION NOT NULL,\n",
    "            longitude DOUBLE PRECISION NOT NULL\n",
    "            \n",
    "        )'''\n",
    "    )\n",
    "    \n",
    "    \n",
    "    postgresql_conn.execute('''\n",
    "        DROP TABLE IF EXISTS growth_schema.products CASCADE;\n",
    "        CREATE TABLE IF NOT EXISTS growth_schema.products (\n",
    "            product_id CHAR(5) PRIMARY KEY,\n",
    "            name VARCHAR(255) NOT NULL,\n",
    "            unit_price DOUBLE PRECISION NOT NULL,\n",
    "            category VARCHAR(255) NOT NULL\n",
    "        )'''\n",
    "    )\n",
    "\n",
    "\n",
    "    postgresql_conn.execute('''\n",
    "        DROP TABLE IF EXISTS growth_schema.orders CASCADE;\n",
    "        CREATE TABLE IF NOT EXISTS growth_schema.orders (\n",
    "            InvoiceNo CHAR(6) NOT NULL,\n",
    "            Quantity INT NOT NULL,\n",
    "            CustomerID CHAR(5) NOT NULL,\n",
    "            ProductID CHAR(5) NOT NULL,\n",
    "            Channel  VARCHAR(10) CHECK(Channel IN ('Website', 'Warehouse')),\n",
    "            InvoiceDate TIMESTAMP NOT NULL,\n",
    "            FOREIGN KEY(CustomerID) REFERENCES growth_schema.customers(customer_id) ON DELETE CASCADE,\n",
    "            FOREIGN KEY(ProductID) REFERENCES growth_schema.products(ProductID) ON DELETE CASCADE\n",
    "        )'''\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_postgresql_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_into_customer():\n",
    "    \n",
    "    \n",
    "    cust_data = pd.read_csv('dataset/customers.csv', encoding='latin1')\n",
    "\n",
    "    # Get unique values in the 'Cities' column for our API usage\n",
    "    unique_values = cust_data['city'].unique()\n",
    "\n",
    "    # Convert to a Pandas Series\n",
    "    unique_series = pd.Series(unique_values)\n",
    "\n",
    "    # Save to CSV\n",
    "    unique_series.to_csv('dataset/uni_cities.csv', index=False, header=['city'])\n",
    "\n",
    "    cust_data['date_of_birth'] = pd.to_datetime(cust_data['date_of_birth'], errors='coerce')\n",
    "\n",
    "    cust_data['date_of_birth'] = cust_data['date_of_birth'].dt.date\n",
    "\n",
    "    dob = pd.to_datetime(cust_data['date_of_birth'])\n",
    "\n",
    "    # Calculate today's date\n",
    "    today = datetime.today()\n",
    "\n",
    "    # Calculate age in days\n",
    "    age_date = ((today - dob).dt.days)\n",
    "\n",
    "    # Convert age to years\n",
    "    cust_data['age'] = (age_date // 365).astype(int)\n",
    "    \n",
    "    \n",
    "    for index, row in cust_data.iterrows():\n",
    "        postgresql_conn.execute('''\n",
    "            INSERT INTO growth_schema.customers (customer_id, first_name, last_name, date_of_birth, email, gender, city, age) \n",
    "            VALUES (%s, %s, %s, %s, %s, %s, %s, %s) ''', \n",
    "            (row['customer_id'], row['first_name'], row['last_name'], row['date_of_birth'], \n",
    "                                    row['email'], row['gender'],row['city'],row['age']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_data_into_customer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_into_products():\n",
    "\n",
    "    products = pd.read_csv('dataset/products.csv')\n",
    "\n",
    "    for index, row in products.iterrows():\n",
    "        postgresql_conn.execute('''\n",
    "            INSERT INTO growth_schema.products (ProductID, name, UnitPrice) \n",
    "            VALUES (%s, %s, %s)''', \n",
    "            (row['ProductID'], row['name'], row['UnitPrice']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_data_into_products()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_data_into_orders():\n",
    "    \n",
    "    orders_df = pd.read_csv('dataset/orders.csv')\n",
    "\n",
    "    for index, row in orders_df.iterrows():\n",
    "        postgresql_conn.execute('''\n",
    "            INSERT INTO growth_schema.orders (InvoiceNo, Quantity, CustomerID, ProductID, Channel, InvoiceDate) \n",
    "            VALUES (%s, %s, %s, %s, %s, %s)''', \n",
    "            (row['InvoiceNo'], row['Quantity'], row['CustomerID'], row['ProductID'], row['Channel'], row['InvoiceDate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_data_into_orders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data():\n",
    "     \n",
    "    # Load customer data\n",
    "    customer_df = pd.read_csv('dataset/customers.csv')\n",
    "\n",
    "    input_data = customer_df[['city', 'latitude', 'longitude']]\n",
    "\n",
    "    unique_cities = input_data.drop_duplicates()\n",
    "\n",
    "    # Initialize an empty DataFrame to store the combined data\n",
    "    final_dataframe = pd.DataFrame()\n",
    "\n",
    "    # Setup the Open-Meteo API client with cache and retry on error\n",
    "    cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
    "    retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "    openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "    for index, row in unique_cities.iterrows():\n",
    "        latitude = row['latitude']\n",
    "        longitude = row['longitude']\n",
    "        \n",
    "        # Check if latitude and longitude are within valid ranges\n",
    "        if -90 <= latitude <= 90 and -180 <= longitude <= 180:\n",
    "        \n",
    "            # Set latitude and longitude in params\n",
    "            params = {\n",
    "                \"latitude\": latitude,\n",
    "                \"longitude\": longitude,\n",
    "                \"start_date\": \"2010-01-01\",\n",
    "                \"end_date\": \"2011-10-31\",\n",
    "                \"hourly\": [\n",
    "                    \"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\",\n",
    "                    \"apparent_temperature\", \"precipitation\", \"rain\", \"snowfall\",\n",
    "                    \"snow_depth\", \"weather_code\", \"pressure_msl\", \"surface_pressure\",\n",
    "                    \"cloud_cover\", \"wind_speed_10m\"\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            # The order of variables in hourly or daily is important to assign them correctly below\n",
    "            url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "            \n",
    "            responses = openmeteo.weather_api(url, params=params)\n",
    "            response = responses[0]\n",
    "\n",
    "            # Process hourly data. The order of variables needs to be the same as requested.\n",
    "            hourly = response.Hourly()\n",
    "            hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "            hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
    "            hourly_dew_point_2m = hourly.Variables(2).ValuesAsNumpy()\n",
    "            hourly_apparent_temperature = hourly.Variables(3).ValuesAsNumpy()\n",
    "            hourly_precipitation = hourly.Variables(4).ValuesAsNumpy()\n",
    "            hourly_rain = hourly.Variables(5).ValuesAsNumpy()\n",
    "            hourly_snowfall = hourly.Variables(6).ValuesAsNumpy()\n",
    "            hourly_snow_depth = hourly.Variables(7).ValuesAsNumpy()\n",
    "            hourly_weather_code = hourly.Variables(8).ValuesAsNumpy()\n",
    "            hourly_pressure_msl = hourly.Variables(9).ValuesAsNumpy()\n",
    "            hourly_surface_pressure = hourly.Variables(10).ValuesAsNumpy()\n",
    "            hourly_cloud_cover = hourly.Variables(11).ValuesAsNumpy()\n",
    "            hourly_wind_speed_10m = hourly.Variables(12).ValuesAsNumpy()\n",
    "\n",
    "            hourly_data = {\"date\": pd.date_range(\n",
    "                start = pd.to_datetime(hourly.Time(), unit = \"s\"),\n",
    "                end = pd.to_datetime(hourly.TimeEnd(), unit = \"s\"),\n",
    "                freq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "                inclusive = \"left\"\n",
    "            )}\n",
    "            hourly_data[\"temp\"] = hourly_temperature_2m\n",
    "            hourly_data[\"rel_hum\"] = hourly_relative_humidity_2m\n",
    "            hourly_data[\"dew_pnt\"] = hourly_dew_point_2m\n",
    "            hourly_data[\"apparent_temp\"] = hourly_apparent_temperature\n",
    "            hourly_data[\"precip\"] = hourly_precipitation\n",
    "            hourly_data[\"rain\"] = hourly_rain\n",
    "            hourly_data[\"snowfall\"] = hourly_snowfall\n",
    "            hourly_data[\"snow_depth\"] = hourly_snow_depth\n",
    "            hourly_data[\"weather_code\"] = hourly_weather_code\n",
    "            hourly_data[\"pressure_msl\"] = hourly_pressure_msl\n",
    "            hourly_data[\"surface_pressure\"] = hourly_surface_pressure\n",
    "            hourly_data[\"cloud_cover\"] = hourly_cloud_cover\n",
    "            hourly_data[\"wind_speed\"] = hourly_wind_speed_10m\n",
    "            hourly_data[\"latitude\"] = latitude\n",
    "            hourly_data[\"longitde\"] = longitude\n",
    "\n",
    "            hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "\n",
    "            # Append the current location's data to the final DataFrame\n",
    "            final_dataframe = pd.concat([final_dataframe, hourly_dataframe], ignore_index=True)\n",
    "\n",
    "            # Add a 1-minute wait before making the next request\n",
    "            time.sleep(60)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "            #print(f\"Skipping invalid coordinates: Latitude={latitude}, Longitude={longitude}\")\n",
    "    final_dataframe.to_csv('dataset/weathers.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_populate_weather_data():\n",
    "        \n",
    "\n",
    "    w_data = pd.read_csv('dataset/weathers.csv')\n",
    "\n",
    "    # create table for weather\n",
    "    postgresql_conn.execute('''\n",
    "                            \n",
    "        DROP TABLE IF EXISTS growth_schema.weather CASCADE;\n",
    "        CREATE TABLE IF NOT EXISTS growth_schema.weather (\n",
    "                            date TIMESTAMP NOT NULL,\n",
    "                            temp DOUBLE PRECISION NOT NULL, \n",
    "                            rel_hum DOUBLE PRECISION NOT NULL, \n",
    "                            dew_pnt DOUBLE PRECISION NOT NULL, \n",
    "                            apparent_temp  DOUBLE PRECISION NOT NULL,\n",
    "                            precip  DOUBLE PRECISION NOT NULL, \n",
    "                            rain DOUBLE PRECISION NOT NULL, \n",
    "                            snowfall DOUBLE PRECISION NOT NULL,\n",
    "                            snow_depth DOUBLE PRECISION NOT NULL, \n",
    "                            weather_code DOUBLE PRECISION NOT NULL,\n",
    "                            pressure_msl DOUBLE PRECISION NOT NULL, \n",
    "                            surface_pressure DOUBLE PRECISION NOT NULL, \n",
    "                            cloud_cover DOUBLE PRECISION NOT NULL, \n",
    "                            wind_speed DOUBLE PRECISION NOT NULL, \n",
    "                            latitude DOUBLE PRECISION NOT NULL, \n",
    "                            longitude DOUBLE PRECISION NOT NULL\n",
    "    )'''\n",
    ")\n",
    "\n",
    "    for index, row in w_data.iterrows():\n",
    "        postgresql_conn.execute('''\n",
    "            INSERT INTO growth_schema.weather (date, temp, rel_hum, dew_pnt, apparent_temp, precip, rain,\n",
    "                                                snowfall, snow_depth, weather_code, pressure_msl,\n",
    "                                                surface_pressure, cloud_cover, wind_speed,latitude,longitude ) \n",
    "                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)''', \n",
    "            \n",
    "            (row['date'], row['temp'], row['rel_hum'], row['dew_pnt'], row['apparent_temp'], row['precip'], row['rain'],\n",
    "                                        row['snowfall'], row['snow_depth'], row['weather_code'], row['pressure_msl'],\n",
    "                                        row['surface_pressure'], row['cloud_cover'], row['wind_speed'], row['latitude'], row['longitude']))\n",
    "\n",
    "\n",
    "    # Close connection\n",
    "    postgresql_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_and_populate_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
